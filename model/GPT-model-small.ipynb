{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e61100-3f0e-4399-853d-133c1677df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0  What was the mortgage price in Corner Brook, N...   \n",
      "1  What was the mortgage price in Gander, Newfoun...   \n",
      "2  What was the mortgage price in Gander, Newfoun...   \n",
      "3  What was the mortgage price in Gander, Newfoun...   \n",
      "4  What was the mortgage price in Labrador City, ...   \n",
      "\n",
      "                                              Answer  \n",
      "0  The mortgage price in Corner Brook, Newfoundla...  \n",
      "1  The mortgage price in Gander, Newfoundland and...  \n",
      "2  The mortgage price in Gander, Newfoundland and...  \n",
      "3  The mortgage price in Gander, Newfoundland and...  \n",
      "4  The mortgage price in Labrador City, Newfoundl...  \n",
      "Question    0\n",
      "Answer      0\n",
      "dtype: int64\n",
      "       Question_length  Answer_length\n",
      "count     68759.000000   68759.000000\n",
      "mean         86.897148      87.979392\n",
      "std           7.828006       7.850963\n",
      "min          73.000000      74.000000\n",
      "25%          81.000000      83.000000\n",
      "50%          85.000000      86.000000\n",
      "75%          91.000000      92.000000\n",
      "max         119.000000     121.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Q&A data from the CSV file\n",
    "qa_data = pd.read_csv('/Users/ernestgaisie/Desktop/Final Projects/CANADA_MORTGAGE_RATES_ANALYSIS/qa_pairs.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(qa_data.head())\n",
    "\n",
    "# Check for any missing values\n",
    "print(qa_data.isnull().sum())\n",
    "\n",
    "# Inspect the distribution of question and answer lengths\n",
    "qa_data['Question_length'] = qa_data['Question'].apply(len)\n",
    "qa_data['Answer_length'] = qa_data['Answer'].apply(len)\n",
    "\n",
    "print(qa_data[['Question_length', 'Answer_length']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b3b09e-f7e8-4e00-a7d6-c8604a65a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Assign the eos_token as the pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22147490-fa67-4ec5-a68f-6cd3c3ebf9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = max(qa_data['Question'].apply(lambda x: len(tokenizer.encode(x))) +\n",
    "                 qa_data['Answer'].apply(lambda x: len(tokenizer.encode(x))))\n",
    "\n",
    "# Tokenize and pad the Q&A pairs\n",
    "qa_pairs = []\n",
    "\n",
    "for q, a in zip(qa_data['Question'], qa_data['Answer']):\n",
    "    encoded_q = tokenizer.encode(q, return_tensors='pt', padding='max_length', max_length=max_length, truncation=True)\n",
    "    encoded_a = tokenizer.encode(a, return_tensors='pt', padding='max_length', max_length=max_length, truncation=True)\n",
    "    qa_pairs.append({'input_ids': encoded_q.squeeze(), 'labels': encoded_a.squeeze()})\n",
    "\n",
    "# Create a custom PyTorch dataset\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __len__(self):\n",
    "        return len(qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return qa_pairs[idx]\n",
    "\n",
    "dataset = QADataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d6f292-2f43-4d40-88e9-b2c3b788112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results2',            # Output directory\n",
    "    per_device_train_batch_size=4,     # Batch size per device\n",
    "    num_train_epochs=1,                # Number of epochs\n",
    "    save_steps=10_000,                 # Save checkpoint every 10 steps\n",
    "    save_total_limit=2,                # Limit the total amount of checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e8fdee-4082-4d1d-951c-a110f05aad7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17190' max='17190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17190/17190 4:50:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.487800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.479300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.473500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.470600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.457200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.458600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.455700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.457400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.457900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17190, training_loss=0.4768675222169389, metrics={'train_runtime': 17437.0669, 'train_samples_per_second': 3.943, 'train_steps_per_second': 0.986, 'total_flos': 2140501774464000.0, 'train_loss': 0.4768675222169389, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a227f4d-c6fc-4234-aefb-bfe33b9dbe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2/tokenizer_config.json',\n",
       " './fine_tuned_gpt2/special_tokens_map.json',\n",
       " './fine_tuned_gpt2/vocab.json',\n",
       " './fine_tuned_gpt2/merges.txt',\n",
       " './fine_tuned_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7dae9ba-818a-4083-b5f4-db55e31a31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the mortgage rate?.,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "# Example input\n",
    "input_text = \"What is the mortgage rate?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids, max_length=30, num_return_sequences=1)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8a8b2-807a-4c59-bd34-c2d96e54a507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
